{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-02T14:42:30.984193Z",
     "start_time": "2026-01-02T14:42:30.908233Z"
    }
   },
   "source": [
    "from genaisys.querying_functions import get_query_results, display_results\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from ipywidgets import Dropdown, Text, Checkbox, Layout, Button, HBox, Output\n",
    "from genaisys import init_openai_api, make_openai_api_call\n",
    "\n",
    "\n",
    "# ============ SHARED STATE ============\n",
    "user_histories = {\"User01\": [], \"User02\": [], \"User03\": []}\n",
    "active_user = \"User01\"\n",
    "conversation_active = True\n",
    "\n",
    "# Create output widgets\n",
    "conversation_output = Output()\n",
    "debug_output = Output()\n",
    "user_memory = True\n",
    "\n",
    "# ============ FUNCTIONS ============\n",
    "\n",
    "def chat_with_gpt(messages, user_message):\n",
    "    try:\n",
    "      namespace=\"\"\n",
    "      if \"Pinecone\" in user_message or \"RAG\" in user_message:\n",
    "         # Determine the keyword\n",
    "        if \"Pinecone\" in user_message:\n",
    "            namespace=\"genaisys\"\n",
    "        elif \"RAG\" in user_message:\n",
    "            namespace=\"data01\"\n",
    "        print(namespace)\n",
    "        #define query text\n",
    "        query_text=user_message\n",
    "        # Retrieve query results\n",
    "        query_results = get_query_results(query_text, namespace)\n",
    "        # Process and display the results\n",
    "        print(\"Processed query results:\")\n",
    "        qtext, target_id = display_results(query_results)\n",
    "        print(qtext)\n",
    "        #run task\n",
    "        sc_input=qtext + \" \" + user_message\n",
    "        mrole = \"system\"\n",
    "        mcontent = \"You are an assistant who executes the tasks you are asked to do.\"\n",
    "        user_role = \"user\"\n",
    "        task_response = make_openai_api_call(sc_input,mrole,mcontent,user_role)\n",
    "        print(task_response)\n",
    "        aug_output=namespace + \":\" +task_response\n",
    "      else:\n",
    "        if user_memory:\n",
    "                # Extract ALL user messages from the conversation history\n",
    "                user_messages_content = [\n",
    "                    msg[\"content\"] for msg in messages\n",
    "                    if msg[\"role\"] == \"user\" and \"content\" in msg\n",
    "                ]\n",
    "                # Combine all extracted user messages into a single string\n",
    "                combined_user_messages = \" \".join(user_messages_content)\n",
    "                # Add the current user_message to the combined text\n",
    "                umessage = f\"{combined_user_messages} {user_message}\"\n",
    "        else:\n",
    "                umessage = user_message\n",
    "        mrole = \"system\"\n",
    "        mcontent = \"You are an assistant who executes the tasks you are asked to do.\"\n",
    "        user_role = \"user\"\n",
    "        task_response = make_openai_api_call(umessage,mrole,mcontent,user_role)\n",
    "        aug_output=task_response\n",
    "      # Return the augmented output\n",
    "      return aug_output\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "def save_conversation_history():\n",
    "    \"\"\"Save conversation history to JSON file.\"\"\"\n",
    "    filename = \"conversation_history.json\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(user_histories, file, indent=4)\n",
    "    display(HTML(f\"<div style='color: green;'><strong>Saved to {filename}</strong></div>\"))\n",
    "\n",
    "\n",
    "def update_display():\n",
    "    \"\"\"Update the conversation display.\"\"\"\n",
    "    global active_user\n",
    "    with debug_output:\n",
    "        print(f\"DEBUG: update_display() - Active user: {active_user}\")\n",
    "        print(f\"DEBUG: History entries: {len(user_histories[active_user])}\")\n",
    "\n",
    "    with conversation_output:\n",
    "        clear_output(wait=True)\n",
    "        for entry in user_histories[active_user]:\n",
    "            if entry['role'] == 'user':\n",
    "                display(HTML(\n",
    "                    f\"<div style='margin-left: 20px; color: blue; margin-bottom: 5px;'>\"\n",
    "                    f\"<strong>{active_user}:</strong> {entry['content']}</div>\"\n",
    "                ))\n",
    "            elif entry['role'] == 'assistant':\n",
    "                display(HTML(\n",
    "                    f\"<div style='margin-left: 20px; color: green; margin-bottom: 5px;'>\"\n",
    "                    f\"<strong>Agent:</strong> {entry['content']}</div>\"\n",
    "                ))\n",
    "        if conversation_active:\n",
    "            display(VBox([user_selector, input_box, agent_checkbox]))  # Keep input box, selector, and checkbox visible if active\n",
    "\n",
    "\n",
    "def chat(user_message):\n",
    "    \"\"\"Handle user input and generate response.\"\"\"\n",
    "    global active_user, conversation_active\n",
    "\n",
    "    with debug_output:\n",
    "        print(f\"DEBUG: chat() called with: '{user_message}'\")\n",
    "\n",
    "    # Check for exit\n",
    "    if user_message.lower() in ['exit', 'quit']:\n",
    "        conversation_active = False\n",
    "        with conversation_output:\n",
    "            clear_output()\n",
    "            display(HTML(\"<div style='color: red;'><strong>Conversation ended.</strong></div>\"))\n",
    "            save_conversation_history()\n",
    "        return\n",
    "\n",
    "    # Add user message to history\n",
    "    user_histories[active_user].append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    # Generate response if agent is enabled\n",
    "    if agent_checkbox.value:\n",
    "        with debug_output:\n",
    "            print(\"DEBUG: Generating agent response...\")\n",
    "        try:\n",
    "            # The user history of this conversation and the user message are sent to the chat_with_gpt()\n",
    "            response = chat_with_gpt(user_histories[active_user], user_message)\n",
    "            with debug_output:\n",
    "                print(f\"DEBUG: Response: '{response[:100]}...'\")\n",
    "            # Append to the history of the response\n",
    "            user_histories[active_user].append({\"role\": \"assistant\", \"content\": response})\n",
    "        except Exception as e:\n",
    "            with debug_output:\n",
    "                print(f\"DEBUG: Error: {e}\")\n",
    "            user_histories[active_user].append({\"role\": \"assistant\", \"content\": f\"Error: {e}\"})\n",
    "    else:\n",
    "        with debug_output:\n",
    "            print(\"DEBUG: Agent disabled, skipping response\")\n",
    "    # Refresh the UI\n",
    "    update_display()\n",
    "\n",
    "\n",
    "def on_user_change(change):\n",
    "    \"\"\"Handle user selection change.\"\"\"\n",
    "    global active_user\n",
    "    active_user = change['new']\n",
    "    with debug_output:\n",
    "        print(f\"DEBUG: User changed to: {active_user}\")\n",
    "    update_display()\n",
    "\n",
    "\n",
    "def handle_input(sender):\n",
    "    \"\"\"Handle input submission (Enter key or button).\"\"\"\n",
    "    user_message = input_box.value\n",
    "    with debug_output:\n",
    "        print(f\"DEBUG: Input received: '{user_message}'\")\n",
    "\n",
    "    if user_message and user_message.strip():\n",
    "        input_box.value = \"\"  # Clear input\n",
    "        chat(user_message)\n",
    "\n",
    "# Function to save conversation history to a file\n",
    "def save_conversation_history():\n",
    "    filename = \"conversation_history.json\"  # Define the filename\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(user_histories, file, indent=4)  # Write the user histories dictionary to the file in JSON format\n",
    "    display(HTML(f\"<div style='color: green;'><strong>Conversation history saved to {filename}.</strong></div>\"))\n",
    "\n",
    "# ============ UI WIDGETS ============\n",
    "\n",
    "user_selector = Dropdown(\n",
    "    options=[\"User01\", \"User02\", \"User03\"],\n",
    "    value=active_user,\n",
    "    description='User:',\n",
    "    layout=Layout(width='200px')\n",
    ")\n",
    "user_selector.observe(on_user_change, names='value')\n",
    "\n",
    "input_box = Text(\n",
    "    placeholder=\"Type your message here and press Enter or click Send\",\n",
    "    layout=Layout(width='400px'),\n",
    "    continuous_update=False\n",
    ")\n",
    "input_box.observe(lambda change: handle_input(change) if change['name'] == 'value' and change['new'] else None, names='value')\n",
    "\n",
    "send_button = Button(\n",
    "    description='Send',\n",
    "    button_style='primary',\n",
    "    layout=Layout(width='80px')\n",
    ")\n",
    "send_button.on_click(lambda b: handle_input(b))\n",
    "\n",
    "agent_checkbox = Checkbox(\n",
    "    value=True,\n",
    "    description='Agent',\n",
    "    layout=Layout(width='100px')\n",
    ")\n",
    "\n",
    "# ============ DISPLAY INTERFACE ============\n",
    "\n",
    "display(HTML(\"<h3>GenAI Chat Interface</h3>\"))\n",
    "display(user_selector)\n",
    "display(HBox([input_box, send_button]))\n",
    "display(agent_checkbox)\n",
    "display(HTML(\"<hr><h4>Conversation:</h4>\"))\n",
    "display(conversation_output)\n",
    "display(HTML(\"<hr><h4>Debug Output:</h4>\"))\n",
    "display(debug_output)\n",
    "\n",
    "with debug_output:\n",
    "    print(\"DEBUG: Interface ready.\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<h3>GenAI Chat Interface</h3>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Dropdown(description='User:', layout=Layout(width='200px'), options=('User01', 'User02', 'User03'), value='Use…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d57073856fa943bab89bb1a03c23f21b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(Text(value='', continuous_update=False, layout=Layout(width='400px'), placeholder='Type your me…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f610dc8c2f3a4dcea497888d59d9bec8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Checkbox(value=True, description='Agent', layout=Layout(width='100px'))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e757e7fe7fb4459da8d9629450fde878"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr><h4>Conversation:</h4>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b56a2584ede4ff290e7f45480300f79"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr><h4>Debug Output:</h4>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d094a66477354138a4c563b967ba03de"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T14:19:55.204517Z",
     "start_time": "2026-01-02T14:19:55.197691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "display_conversation_history=True\n",
    "summary=True\n",
    "import os\n",
    "\n",
    "if display_conversation_history == True or summary == True:\n",
    "    file_path = \"conversation_history.json\"\n",
    "    if os.path.exists(file_path):\n",
    "        display_conversation_history = True\n",
    "        summary = True\n",
    "        print(f\"The file {file_path} exists.\")\n",
    "    else:\n",
    "        display_conversation_history = False\n",
    "        summary = False\n",
    "        print(f\"The file {file_path} does not exist.\")\n",
    "        print(\"The conversation history will not be processed\")\n"
   ],
   "id": "a713d7f0060549d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file conversation_history.json exists.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T14:45:32.001465Z",
     "start_time": "2026-01-02T14:45:20.836647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.core.display import Markdown\n",
    "# Load and summarize the conversation history\n",
    "# Display option\n",
    "display_conversation_history = True\n",
    "if display_conversation_history:\n",
    "  # File path\n",
    "  file_path = 'conversation_history.json'\n",
    "  # Open the file and read its content into the 'dialog' variable\n",
    "  with open(file_path, 'r', encoding='utf-8') as file:\n",
    "      dialog = json.load(file)  # Parse JSON content\n",
    "  # Function to format JSON content as markdown\n",
    "def summarize_conversation(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        dialog = file.read()\n",
    "    conversation_history_json = json.loads(dialog)\n",
    "    # Step 2: Construct dialog string from the JSON conversation history\n",
    "    def construct_dialog(conversation_history_json):\n",
    "        dialog = \"\"\n",
    "        for user, messages in conversation_history_json.items():\n",
    "            dialog += f\"\\n{user}:\\n\"\n",
    "            for message in messages:\n",
    "                role = message[\"role\"]\n",
    "                content = message[\"content\"]\n",
    "                dialog += f\"- {role}: {content}\\n\"\n",
    "        return dialog\n",
    "    formatted_dialog = construct_dialog(conversation_history_json)\n",
    "    # Step 3: Prepare the task for the summary\n",
    "    mrole = \"system\"\n",
    "    mcontent = \"Your task is to read this JSON formatted text and summarize it.\"\n",
    "    user_role = \"user\"\n",
    "    task = f\"Read this JSON formatted text and make a very detailed summary of it with a list of actions:\\n{formatted_dialog}\"\n",
    "    # Step 4: Call the `make_openai_api_call` function\n",
    "    task_response = make_openai_api_call(task, mrole, mcontent, user_role)\n",
    "    # Step 5: Display the task response as Markdown\n",
    "    display(Markdown(task_response))\n",
    "\n",
    "if summary:\n",
    "    # File path to the JSON file\n",
    "    file_path = 'conversation_history.json'\n",
    "    # Check if the file exists before calling the function\n",
    "    if os.path.exists(file_path):\n",
    "        summarize_conversation(file_path)\n",
    "    else:\n",
    "        print(f\"File '{file_path}' does not exist. Please provide a valid file path.\")\n"
   ],
   "id": "2dff086854dfa4a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The JSON text involves a conversation between a user and an AI assistant regarding customer feedback and the application of AI memory systems to improve a travel agency's services. Here's a detailed summary with a list of actions:\n\n### Summary:\n\n1. **Customer Feedback Analysis:**\n   - A customer expressed dissatisfaction with the travel agency, comparing it unfavorably to competitors and suggesting service improvements. The sentiment analysis score was low (0.2), indicating negative sentiment.\n\n2. **Understanding AI Memory Systems:**\n   - The user inquired about the CTO's mention of leveraging different kinds of memories, which was explained in the context of Retrieval-Augmented Generation (RAG). The AI assistant detailed various memory types:\n     - **Short-term Memory:** Immediate context for processing queries.\n     - **Long-term Memory:** Persistent storage for future access.\n     - **Episodic Memory:** Storage of specific events or experiences.\n     - **Semantic Memory:** General knowledge and facts.\n     - **Procedural Memory:** Skills and procedures.\n\n3. **Application in Travel Promotion Campaigns:**\n   - The AI assistant suggested strategies to leverage these memory types to enhance the travel agency's services:\n     - **Personalized Recommendations:** Use episodic memory for tailored travel suggestions.\n     - **Enhanced Customer Support:** Implement RAG for accurate responses.\n     - **Content Generation:** Use semantic memory for informative content.\n     - **Automated Processes:** Utilize procedural memory for routine tasks.\n     - **Feedback Analysis:** Use AI to identify and address common issues.\n\n4. **Next Steps:**\n   - The user decided to summarize the discussion and consult with the manager to seek approval for implementing these strategies.\n\n### Actions:\n\n1. **Conduct Sentiment Analysis:**\n   - Regularly analyze customer feedback to gauge sentiment and identify areas for improvement.\n\n2. **Explore AI Memory Systems:**\n   - Investigate the use of RAG and different memory types to enhance AI capabilities in customer interactions.\n\n3. **Develop Personalized Campaigns:**\n   - Implement strategies for personalized travel recommendations and content generation.\n\n4. **Enhance Customer Support:**\n   - Use AI to provide more accurate and contextually relevant responses to customer inquiries.\n\n5. **Automate Routine Tasks:**\n   - Utilize AI to automate booking confirmations, itinerary updates, and follow-up communications.\n\n6. **Analyze Feedback:**\n   - Continuously analyze customer feedback to refine services and address concerns proactively.\n\n7. **Consult Management:**\n   - Present the proposed strategies to the manager for approval and potential implementation."
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
